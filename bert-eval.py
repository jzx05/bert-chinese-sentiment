import os
import json
import time
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
import logging

import torch
from transformers import BertTokenizer, BertForSequenceClassification, pipeline
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class EvaluationConfig:
    """è¯„ä¼°é…ç½®ç±»"""
    model_path: str = r"E:\Codes\bert-base-chinese\sentiment_model"
    device: str = 'cpu'  # 'cpu' æˆ– 'cuda'
    batch_size: int = 32
    max_length: int = 512
    save_results: bool = True
    output_dir: str = "./evaluation_results"

class SentimentAnalyzer:
    """æƒ…æ„Ÿåˆ†æå™¨ç±»"""
    
    def __init__(self, config: EvaluationConfig):
        self.config = config
        self.tokenizer = None
        self.model = None
        self.classifier = None
        self.label_map = {0: "è´Ÿé¢", 1: "æ­£é¢"}
        
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        try:
            logger.info(f"åŠ è½½æ¨¡å‹ä»: {self.config.model_path}")
            
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„
            if not os.path.exists(self.config.model_path):
                raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.config.model_path}")
            
            # åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
            self.tokenizer = BertTokenizer.from_pretrained(self.config.model_path)
            self.model = BertForSequenceClassification.from_pretrained(
                self.config.model_path, 
                num_labels=2
            )
            
            # åˆ›å»ºpipeline
            self.classifier = pipeline(
                "text-classification", 
                model=self.model, 
                tokenizer=self.tokenizer, 
                device=self.config.device,
                batch_size=self.config.batch_size
            )
            
            logger.info("æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def analyze_single_text(self, text: str) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ–‡æœ¬"""
        try:
            start_time = time.time()
            result = self.classifier(text)
            
            # è§£æç»“æœ
            label_id = int(result[0]['label'].replace('LABEL_', ''))
            label = self.label_map[label_id]
            confidence = result[0]['score']
            processing_time = time.time() - start_time
            
            return {
                'text': text,
                'label': label,
                'label_id': label_id,
                'confidence': confidence,
                'processing_time': processing_time
            }
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬åˆ†æå¤±è´¥: {text}, é”™è¯¯: {e}")
            return {
                'text': text,
                'label': 'é”™è¯¯',
                'label_id': -1,
                'confidence': 0.0,
                'processing_time': 0.0,
                'error': str(e)
            }
    
    def analyze_batch(self, texts: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡åˆ†ææ–‡æœ¬"""
        logger.info(f"å¼€å§‹æ‰¹é‡åˆ†æ {len(texts)} ä¸ªæ–‡æœ¬...")
        
        results = []
        for i, text in enumerate(texts):
            if i % 100 == 0:
                logger.info(f"è¿›åº¦: {i}/{len(texts)}")
            
            result = self.analyze_single_text(text)
            results.append(result)
        
        logger.info("æ‰¹é‡åˆ†æå®Œæˆï¼")
        return results
    
    def get_performance_stats(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—æ€§èƒ½ç»Ÿè®¡"""
        valid_results = [r for r in results if r['label_id'] != -1]
        
        if not valid_results:
            return {}
        
        # åŸºç¡€ç»Ÿè®¡
        total_texts = len(valid_results)
        avg_confidence = np.mean([r['confidence'] for r in valid_results])
        avg_processing_time = np.mean([r['processing_time'] for r in valid_results])
        
        # æ ‡ç­¾åˆ†å¸ƒ
        label_counts = {}
        for r in valid_results:
            label = r['label']
            label_counts[label] = label_counts.get(label, 0) + 1
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        confidence_ranges = {
            'é«˜ç½®ä¿¡åº¦ (>0.8)': len([r for r in valid_results if r['confidence'] > 0.8]),
            'ä¸­ç½®ä¿¡åº¦ (0.6-0.8)': len([r for r in valid_results if 0.6 <= r['confidence'] <= 0.8]),
            'ä½ç½®ä¿¡åº¦ (<0.6)': len([r for r in valid_results if r['confidence'] < 0.6])
        }
        
        return {
            'total_texts': total_texts,
            'avg_confidence': avg_confidence,
            'avg_processing_time': avg_processing_time,
            'label_distribution': label_counts,
            'confidence_distribution': confidence_ranges
        }

def create_test_cases() -> Tuple[List[str], List[str]]:
    """åˆ›å»ºæµ‹è¯•ç”¨ä¾‹"""
    
    # åŸºç¡€æµ‹è¯•ç”¨ä¾‹
    basic_texts = [
        # è´Ÿé¢æƒ…æ„Ÿ
        "æˆ‘ä»Šå¤©å¿ƒæƒ…å¾ˆç³Ÿç³•",
        "è¿™éƒ¨ç”µå½±å¤ªæ— èŠäº†ï¼Œæµªè´¹æ—¶é—´",
        "æœåŠ¡æ€åº¦å¾ˆå·®ï¼Œä¸ä¼šå†æ¥äº†",
        "äº§å“è´¨é‡å¤ªå·®äº†ï¼Œå¾ˆå¤±æœ›",
        "ä»Šå¤©å·¥ä½œç‰¹åˆ«ç´¯ï¼Œæƒ³ä¼‘æ¯",
        "è¿™ä¸ªå†³å®šå¤ªæ„šè ¢äº†",
        "äº¤é€šå µå¡è®©äººçƒ¦èº",
        "ä»·æ ¼å¤ªè´µäº†ï¼Œä¸å€¼å¾—",
        
        # æ­£é¢æƒ…æ„Ÿ
        "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œå¿ƒæƒ…æ„‰å¿«",
        "è¿™å®¶é¤å…çš„èœå¾ˆå¥½åƒï¼Œæ¨èï¼",
        "å­¦ä¹ æ–°çŸ¥è¯†å¾ˆæœ‰è¶£",
        "æœ‹å‹èšä¼šå¾ˆå¼€å¿ƒ",
        "å·¥ä½œé¡ºåˆ©å®Œæˆï¼Œå¾ˆæœ‰æˆå°±æ„Ÿ",
        "è¿™ä¸ªæƒ³æ³•å¾ˆæ£’",
        "é£æ™¯å¤ªç¾äº†ï¼Œè®©äººå¿ƒæ—·ç¥æ€¡",
        "å­©å­å¾ˆèªæ˜ï¼Œå­¦ä¹ è¿›æ­¥å¾ˆå¿«"
    ]
    
    # å¤æ‚æƒ…æ„Ÿæµ‹è¯•ç”¨ä¾‹
    complex_texts = [
        "è™½ç„¶ä»Šå¤©ä¸‹é›¨äº†ï¼Œä½†æ˜¯å’Œæœ‹å‹èŠå¤©å¾ˆå¼€å¿ƒ",
        "è¿™éƒ¨ç”µå½±å‰§æƒ…ä¸€èˆ¬ï¼Œä½†æ˜¯æ¼”å‘˜æ¼”æŠ€ä¸é”™",
        "å·¥ä½œå¾ˆå¿™å¾ˆç´¯ï¼Œä½†æ˜¯çœ‹åˆ°æˆæœå¾ˆæœ‰æˆå°±æ„Ÿ",
        "ä»·æ ¼æœ‰ç‚¹è´µï¼Œä½†æ˜¯è´¨é‡ç¡®å®å¾ˆå¥½",
        "å­¦ä¹ å¾ˆè¾›è‹¦ï¼Œä½†æ˜¯ä¸ºäº†æ¢¦æƒ³å€¼å¾—",
        "å¤©æ°”ä¸å¤ªå¥½ï¼Œä½†æ˜¯å¿ƒæƒ…è¿˜ä¸é”™",
        "è·¯ç¨‹æœ‰ç‚¹è¿œï¼Œä½†æ˜¯é£æ™¯å¾ˆç¾",
        "å¼€å§‹æœ‰ç‚¹å›°éš¾ï¼Œä½†æ˜¯æ…¢æ…¢å°±é€‚åº”äº†"
    ]
    
    return basic_texts, complex_texts

def save_results(results: List[Dict[str, Any]], config: EvaluationConfig):
    """ä¿å­˜è¯„ä¼°ç»“æœ"""
    if not config.save_results:
        return
    
    os.makedirs(config.output_dir, exist_ok=True)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_file = os.path.join(config.output_dir, "evaluation_results.json")
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜æ€§èƒ½ç»Ÿè®¡
    stats = SentimentAnalyzer(config).get_performance_stats(results)
    stats_file = os.path.join(config.output_dir, "performance_stats.json")
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {config.output_dir}")

def print_results(results: List[Dict[str, Any]], title: str):
    """æ‰“å°è¯„ä¼°ç»“æœ"""
    print(f"\n{'='*60}")
    print(f"{title:^60}")
    print(f"{'='*60}")
    
    for i, result in enumerate(results, 1):
        if 'error' in result:
            print(f"{i:2d}. âŒ {result['text']}")
            print(f"    é”™è¯¯: {result['error']}")
        else:
            emoji = "ğŸ˜" if result['label'] == "è´Ÿé¢" else "ğŸ˜Š"
            print(f"{i:2d}. {emoji} {result['text']}")
            print(f"    æƒ…æ„Ÿ: {result['label']}")
            print(f"    ç½®ä¿¡åº¦: {result['confidence']:.3f}")
            print(f"    å¤„ç†æ—¶é—´: {result['processing_time']:.3f}s")
        print()

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    config = EvaluationConfig()
    
    try:
        # åˆ›å»ºåˆ†æå™¨
        analyzer = SentimentAnalyzer(config)
        
        # è·å–æµ‹è¯•ç”¨ä¾‹
        basic_texts, complex_texts = create_test_cases()
        
        # åˆ†æåŸºç¡€æ–‡æœ¬
        logger.info("å¼€å§‹åŸºç¡€æƒ…æ„Ÿåˆ†æ...")
        basic_results = analyzer.analyze_batch(basic_texts)
        print_results(basic_results, "åŸºç¡€æƒ…æ„Ÿåˆ†æç»“æœ")
        
        # åˆ†æå¤æ‚æ–‡æœ¬
        logger.info("å¼€å§‹å¤æ‚æƒ…æ„Ÿåˆ†æ...")
        complex_results = analyzer.analyze_batch(complex_texts)
        print_results(complex_results, "å¤æ‚æƒ…æ„Ÿåˆ†æç»“æœ")
        
        # è®¡ç®—æ€§èƒ½ç»Ÿè®¡
        all_results = basic_results + complex_results
        stats = analyzer.get_performance_stats(all_results)
        
        # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
        print(f"\n{'='*60}")
        print("æ€§èƒ½ç»Ÿè®¡")
        print(f"{'='*60}")
        print(f"æ€»æ–‡æœ¬æ•°: {stats['total_texts']}")
        print(f"å¹³å‡ç½®ä¿¡åº¦: {stats['avg_confidence']:.3f}")
        print(f"å¹³å‡å¤„ç†æ—¶é—´: {stats['avg_processing_time']:.3f}s")
        print(f"\næ ‡ç­¾åˆ†å¸ƒ:")
        for label, count in stats['label_distribution'].items():
            print(f"  {label}: {count}")
        print(f"\nç½®ä¿¡åº¦åˆ†å¸ƒ:")
        for range_name, count in stats['confidence_distribution'].items():
            print(f"  {range_name}: {count}")
        
        # ä¿å­˜ç»“æœ
        save_results(all_results, config)
        
        logger.info("è¯„ä¼°å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise

if __name__ == "__main__":
    main()





